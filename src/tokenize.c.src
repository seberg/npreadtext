
#include <Python.h>

#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <string.h>

#include "typedefs.h"
#include "stream.h"

#include "tokenize.h"
#include "error_types.h"
#include "parser_config.h"

#include "numpy/ndarraytypes.h"


/*
    How parsing quoted fields works:

    For quoting to be activated, the first character of the field
    must be the quote character (after taking into account
    ignore_leading_spaces).  While quoting is active, delimiters
    are treated as regular characters, not delimiters.  Quoting is
    deactivated by the second occurrence of the quote character.  An
    exception is the occurrence of two consecutive quote characters,
    which is treated as a literal occurrence of a single quote character.
    E.g. (with delimiter=',' and quote='"'):
        12.3,"New York, NY","3'2"""
    The second and third fields are `New York, NY` and `3'2"`.

    If a non-delimiter occurs after the closing quote, the quote is
    ignored and parsing continues with quoting deactivated.  Quotes
    that occur while quoting is not activated are not handled specially;
    they become part of the data.
    E.g:
        12.3,"ABC"DEF,XY"Z
    The second and third fields are `ABCDEF` and `XY"Z`.

    Note that the second field of
        12.3,"ABC"   ,4.5
    is `ABC   `.  Currently there is no option to ignore whitespace
    at the end of a field.
*/


static size_t
next_size(size_t size) {
    return ((size_t)size + 3) & ~(size_t)3;
}

/**begin repeat
 * #type = Py_UCS1, Py_UCS2, Py_UCS4#
 */
static int
copy_to_field_buffer_@type@(tokenizer_state *ts,
        const @type@ *chunk_start, const @type@ *chunk_end)
{
    size_t chunk_length = chunk_end - chunk_start;
    size_t size = chunk_length + ts->field_buffer_pos + 2;

    if (NPY_UNLIKELY(ts->field_buffer_length < size)) {
        size = next_size(size);
        char32_t *grown = PyMem_Realloc(ts->field_buffer, size * sizeof(char32_t));
        if (grown == NULL) {
            PyErr_NoMemory();
            return -1;
        }
        ts->field_buffer_length = size;
        ts->field_buffer = grown;
    }

    char32_t *write_pos = ts->field_buffer + ts->field_buffer_pos;
    for (; chunk_start < chunk_end; chunk_start++, write_pos++) {
        *write_pos = (char32_t)*chunk_start;
    }
    *write_pos = '\0';  /* always ensure we end with NUL */
    ts->field_buffer_pos += chunk_length;
    return 0;
}
/**end repeat**/


static NPY_INLINE int
add_field(tokenizer_state *ts)
{
    /* The previous field is done, advance to keep a NUL byte at the end */
    ts->field_buffer_pos += 1;

    size_t size = ts->num_fields + 1;
    if (NPY_UNLIKELY(size > ts->fields_size)) {
        size = next_size(size);
        field_info *fields = PyMem_Realloc(ts->fields, size * sizeof(*fields));
        if (fields == NULL) {
            PyErr_NoMemory();
            return -1;
        }
        ts->fields = fields;
        ts->fields_size = size;
    }

    ts->fields[ts->num_fields].offset = ts->field_buffer_pos;
    ts->fields[ts->num_fields].quoted = false;
    ts->num_fields += 1;
    /* Ensure this (currently empty) word is NUL terminated. */
    ts->field_buffer[ts->field_buffer_pos] = '\0';
    return 0;
}


/**begin repeat
 * #kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND, PyUnicode_4BYTE_KIND#
 * #type = Py_UCS1, Py_UCS2, Py_UCS4#
 */
static NPY_INLINE int
tokenizer_core_@type@(tokenizer_state *ts, parser_config *const config)
{
    @type@ *pos = (@type@ *)ts->pos;
    @type@ *stop = (@type@ *)ts->end;
    @type@ *chunk_start;

    if (ts->state == TOKENIZE_CHECK_QUOTED) {
        /* before we can check for quotes, strip leading whitespace */
        if (config->ignore_leading_whitespace) {
            while (pos < stop && Py_UNICODE_ISSPACE(*pos) &&
                        *pos != '\r' && *pos != '\n') {
                pos++;
            }
            if (pos == stop) {
                ts->pos = (char *)pos;
                return 0;
            }
        }

        /* Setting chunk effectively starts the field */
        if (*pos == config->quote) {
            ts->fields[ts->num_fields - 1].quoted = true;
            ts->state = TOKENIZE_QUOTED;
            pos++;  /* TOKENIZE_QUOTED is OK with pos == stop */
        }
        else {
            /* Set to TOKENIZE_QUOTED or TOKENIZE_QUOTED_WHITESPACE */
            ts->state = ts->unquoted_state;
        }
    }

    switch (ts->state) {
        case TOKENIZE_UNQUOTED:
            chunk_start = pos;
            for (; pos < stop; pos++) {
                if (!BLOOM(ts->bloom_mask_unquoted, *pos)) {
                    continue;
                }
                if (*pos == '\n') {
                    ts->state = TOKENIZE_EAT_CRLF;
                    break;
                }
                else if (*pos == '\r') {
                    ts->state = TOKENIZE_LINE_END;
                    break;
                }
                else if (*pos == config->delimiter) {
                    ts->state = TOKENIZE_INIT;
                    break;
                }
                else if (*pos == config->comment[0]) {
                    if (config->comment[1] != '\0') {
                        ts->state = TOKENIZE_CHECK_COMMENT;
                        break;
                    }
                    else {
                        ts->state = TOKENIZE_GOTO_LINE_END;
                        break;
                    }
                }
            }
            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {
                return -1;
            }
            pos++;
            break;

        case TOKENIZE_UNQUOTED_WHITESPACE:
            /* Note, this branch is largely identical to `TOKENIZE_UNQUOTED` */
            chunk_start = pos;
            for (; pos < stop; pos++) {
                if (!BLOOM(ts->bloom_mask_unquoted_whitespace, *pos)) {
                    continue;
                }
                if (*pos == '\n') {
                    ts->state = TOKENIZE_EAT_CRLF;
                    break;
                }
                else if (*pos == '\r') {
                    ts->state = TOKENIZE_LINE_END;
                    break;
                }
                else if (Py_UNICODE_ISSPACE(*pos)) {
                    ts->state = TOKENIZE_INIT;
                    break;
                }
                else if (*pos == config->comment[0]) {
                    if (config->comment[1] != '\0') {
                        ts->state = TOKENIZE_CHECK_COMMENT;
                        break;
                    }
                    else {
                        ts->state = TOKENIZE_GOTO_LINE_END;
                        break;
                    }
                }
            }
            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {
                return -1;
            }
            pos++;
            break;

        case TOKENIZE_QUOTED:
            chunk_start = pos;
            for (; pos < stop; pos++) {
                if (!BLOOM(ts->bloom_mask_quoted, *pos)) {
                    continue;
                }
                if (!config->allow_embedded_newline) {
                    if (*pos == '\n') {
                        ts->state = TOKENIZE_EAT_CRLF;
                        break;
                    }
                    else if (*pos == '\r') {
                        ts->state = TOKENIZE_LINE_END;
                        break;
                    }
                }
                else if (*pos != config->quote) {
                    /* inside the field, nothing to do. */
                }
                else {
                    ts->state = TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE;
                    break;
                }
            }
            if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {
                return -1;
            }
            pos++;
            break;

        case TOKENIZE_CHECK_COMMENT:
            if (*pos == config->comment[1]) {
                ts->state = TOKENIZE_GOTO_LINE_END;
                pos++;
            }
            else {
                /* Not a comment, must be tokenizing unquoted now */
                ts->state = TOKENIZE_UNQUOTED;
                /* Copy comment as a chunk to the current field */
                if (copy_to_field_buffer_Py_UCS4(ts,
                        config->comment, config->comment+1) < 0) {
                    return -1;
                }
            }
            break;

        case TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE:
            if (*pos == config->quote) {
                ts->state = TOKENIZE_QUOTED;
                pos++;
            }
            else {
                /* continue parsing as if unquoted */
                ts->state = TOKENIZE_UNQUOTED;
            }
            break;

        case TOKENIZE_GOTO_LINE_END:
            if (ts->buf_state != BUFFER_MAY_CONTAIN_NEWLINE) {
                pos = (@type@ *)ts->end;  /* advance to next buffer */
                ts->state = TOKENIZE_LINE_END;
            }
            for (; pos < stop; pos++) {
                if (*pos == '\r') {
                    ts->state = TOKENIZE_EAT_CRLF;
                    break;
                }
                else if (*pos == '\n') {
                    ts->state = TOKENIZE_LINE_END;
                }
            }
            break;

        case TOKENIZE_EAT_CRLF:
            /* "Universal newline" support: remove \n in \r\n. */
            if (*pos == '\n') {
                pos++;
            }
            ts->state = TOKENIZE_LINE_END;

        default:
            assert(0);
    }

    ts->pos = (char *)pos;
    return 0;
}
/**end repeat**/


/*
 * This version now always copies the full "row" (all tokens).  This makes
 * two things easier:
 * 1. It means that every word is guaranteed to be followed by a NUL character
 *    (although it can include one as well).
 * 2. In the usecols case we can sniff the first row easier by parsing it
 *    fully.
 *
 * The tokenizer could grow the ability to skip fields and check the
 * maximum number of fields when known.
 *
 * Unlike other tokenizers, this one tries to work in chunks and copies
 * data to words only when it it has to.  The hope is that this makes multiple
 * light-weight loops rather than a single heavy one, to allow e.g. quickly
 * scanning for the end of a field.
 */
int
tokenize(stream *s, tokenizer_state *ts, parser_config *const config)
{
    assert(ts->fields_size >= 2);
    assert(ts->field_buffer_length >= 2*sizeof(char32_t));

    int finished_reading_file = 0;

    /* Reset to start of buffer */
    ts->field_buffer_pos = 0;
    ts->num_fields = 0;
    /* Add the first field */

    while (1) {
        if (ts->state == TOKENIZE_INIT) {
            /* Start a new field */
            if (add_field(ts) < 0) {
                return -1;
            }
            ts->state = TOKENIZE_CHECK_QUOTED;
        }

        if (NPY_UNLIKELY(ts->pos >= ts->end)) {
            if (ts->buf_state == BUFFER_IS_LINEND &&
                    ts->state == TOKENIZE_EAT_CRLF) {
                /* Finished line, do not read anymore (also do not eat \n) */
                goto finish;
            }
            /* fetch new data */
            ts->buf_state = stream_nextbuf(s,
                    &ts->pos, &ts->end, &ts->unicode_kind);
            if (ts->buf_state < 0) {
                return -1;
            }
            if (ts->buf_state == BUFFER_IS_FILEEND) {
                finished_reading_file = 1;
                ts->pos = ts->end;  /* should be guaranteed, but make sure. */
                goto finish;
            }
            else if (ts->pos == ts->end) {
                if (ts->buf_state != BUFFER_IS_LINEND) {
                    PyErr_SetString(PyExc_RuntimeError,
                            "Reader returned an empty buffer, "
                            "but did not indicate file or line end.");
                    return -1;
                }
                /* Otherwise, we are OK with this and assume an empty line. */
                goto finish;
            }
        }
        int status;
        if (ts->unicode_kind == PyUnicode_1BYTE_KIND) {
            status = tokenizer_core_Py_UCS1(ts, config);
        }
        else if (ts->unicode_kind == PyUnicode_2BYTE_KIND) {
            status = tokenizer_core_Py_UCS2(ts, config);
        }
        else {
            assert(ts->unicode_kind == PyUnicode_4BYTE_KIND);
            status = tokenizer_core_Py_UCS4(ts, config);
        }
        if (status < 0) {
            return -1;
        }

        if (ts->state == TOKENIZE_LINE_END) {
            goto finish;
        }
    }

  finish:
    /* Finish the last field */
    if (add_field(ts) < 0) {
        return -1;
    }
    ts->num_fields -= 1;
    /*
     * If have one field, but that field is completely empty, this is an
     * empty line, and we just ignore it.
     */
    if (ts->num_fields == 1
             && ts->fields[1].offset - ts->fields[0].offset == 1
             && !ts->fields->quoted) {
        ts->num_fields--;
    }
    ts->state = TOKENIZE_INIT;
    return finished_reading_file;
}


void
tokenizer_clear(tokenizer_state *ts)
{
    PyMem_FREE(ts->field_buffer);
    ts->field_buffer = NULL;
    ts->field_buffer_length = 0;

    PyMem_FREE(ts->fields);
    ts->fields = NULL;
    ts->fields_size = 0;
}


/*
 * The following bloom filter code us borrowed from Python (3.10)
 * `unicodeobject.c`. (As well as the defines in `tokenize.h`)
 */

/* --- Bloom Filters ----------------------------------------------------- */

/* stuff to implement simple "bloom filters" for Unicode characters.
   to keep things simple, we use a single bitmask, using the least 5
   bits from each unicode characters as the bit index. */

/* the linebreak mask is set up by _PyUnicode_Init() below */

static inline BLOOM_MASK
make_bloom_mask(int kind, const void* ptr, Py_ssize_t len)
{
#define BLOOM_UPDATE(TYPE, MASK, PTR, LEN)             \
    do {                                               \
        TYPE *data = (TYPE *)PTR;                      \
        TYPE *end = data + LEN;                        \
        Py_UCS4 ch;                                    \
        for (; data != end; data++) {                  \
            ch = *data;                                \
            MASK |= (1UL << (ch & (BLOOM_WIDTH - 1))); \
        }                                              \
        break;                                         \
    } while (0)

    /* calculate simple bloom-style bitmask for a given unicode string */

    BLOOM_MASK mask;

    mask = 0;
    switch (kind) {
        case PyUnicode_1BYTE_KIND:
            BLOOM_UPDATE(Py_UCS1, mask, ptr, len);
            break;
        case PyUnicode_2BYTE_KIND:
            BLOOM_UPDATE(Py_UCS2, mask, ptr, len);
            break;
        case PyUnicode_4BYTE_KIND:
            BLOOM_UPDATE(Py_UCS4, mask, ptr, len);
            break;
        default:
            Py_UNREACHABLE();
    }
    return mask;

#undef BLOOM_UPDATE
}


/*
 * Initialize the tokenizer.  We may want to copy all important config
 * variables into the tokenizer.  This would improve the cache locality during
 * tokenizing.
 */
int
tokenizer_init(tokenizer_state *ts, parser_config *config)
{
    /* State and buf_state could be moved into tokenize if we go by row */
    ts->buf_state = BUFFER_MAY_CONTAIN_NEWLINE;
    ts->state = TOKENIZE_INIT;

    /* All whitespaces (based on Python) with \r and \n ommited */
    Py_UCS4 ws_buf[] = {
            0x0009, 0x000B, 0x000C, 0x001C, 0x001D, 0x001E, 0x001F, 0x0020,
            0x0085, 0x00A0, 0x1680, 0x2000, 0x2001, 0x2002, 0x2003, 0x2004,
            0x2005, 0x2006, 0x2007, 0x2008, 0x2009, 0x200A, 0x2028, 0x2029,
            0x202F, 0x205F, 0x3000};

    Py_UCS4 buf[4];
    buf[0] = '\n';
    buf[1] = '\r';
    buf[3] = config->delimiter;
    buf[4] = config->comment[0];

    ts->bloom_mask_unquoted = make_bloom_mask(
            PyUnicode_4BYTE_KIND, buf, 4);
    buf[3] = config->quote;
    ts->bloom_mask_quoted = make_bloom_mask(
            PyUnicode_4BYTE_KIND, buf, 3);

    ts->bloom_mask_whitespace = make_bloom_mask(
            PyUnicode_4BYTE_KIND, ws_buf, sizeof(ws_buf) / sizeof(Py_UCS4));
    ts->bloom_mask_unquoted_whitespace = ts->bloom_mask_whitespace;
    buf[0] = config->quote;
    ts->bloom_mask_unquoted_whitespace |= make_bloom_mask(
            PyUnicode_4BYTE_KIND, buf, 1);

    if (config->delimiter_is_whitespace) {
        ts->unquoted_state = TOKENIZE_UNQUOTED_WHITESPACE;
    }
    else {
        ts->unquoted_state = TOKENIZE_UNQUOTED;
    }
    ts->num_fields = 0;

    ts->buf_state = 0;
    ts->pos = NULL;
    ts->end = NULL;

    ts->field_buffer = PyMem_Malloc(32 * sizeof(char32_t));
    if (ts->field_buffer == NULL) {
        PyErr_NoMemory();
        return -1;
    }
    ts->field_buffer_length = 32;

    ts->fields = PyMem_Malloc(5 * sizeof(*ts->fields));
    if (ts->fields == NULL) {
        PyErr_NoMemory();
        return -1;
    }
    ts->fields_size = 5;
    return 0;
}
