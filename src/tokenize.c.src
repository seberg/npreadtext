
#include <Python.h>

#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <string.h>

#include "typedefs.h"
#include "stream.h"

#include "tokenize.h"
#include "error_types.h"
#include "parser_config.h"

#include "numpy/ndarraytypes.h"

/*
    How parsing quoted fields works:

    For quoting to be activated, the first character of the field
    must be the quote character (after taking into account
    ignore_leading_spaces).  While quoting is active, delimiters
    are treated as regular characters, not delimiters.  Quoting is
    deactivated by the second occurrence of the quote character.  An
    exception is the occurrence of two consecutive quote characters,
    which is treated as a literal occurrence of a single quote character.
    E.g. (with delimiter=',' and quote='"'):
        12.3,"New York, NY","3'2"""
    The second and third fields are `New York, NY` and `3'2"`.

    If a non-delimiter occurs after the closing quote, the quote is
    ignored and parsing continues with quoting deactivated.  Quotes
    that occur while quoting is not activated are not handled specially;
    they become part of the data.
    E.g:
        12.3,"ABC"DEF,XY"Z
    The second and third fields are `ABCDEF` and `XY"Z`.

    Note that the second field of
        12.3,"ABC"   ,4.5
    is `ABC   `.  Currently there is no option to ignore whitespace
    at the end of a field.
*/


static size_t
next_size(size_t size) {
    return ((size_t)size + 3) & ~(size_t)3;
}

/**begin repeat
 * #type = Py_UCS1, Py_UCS2, Py_UCS4#
 */
static int
copy_to_field_buffer_@type@(tokenizer_state *ts,
        const @type@ *chunk_start, const @type@ *chunk_end)
{
    size_t chunk_length = chunk_end - chunk_start;
    size_t size = chunk_length + ts->field_buffer_pos + 2;

    if (NPY_UNLIKELY(ts->field_buffer_length < size)) {
        size = next_size(size);
        char32_t *grown = PyMem_Realloc(ts->field_buffer, size * sizeof(char32_t));
        if (grown == NULL) {
            PyErr_NoMemory();
            return -1;
        }
        ts->field_buffer_length = size;
        ts->field_buffer = grown;
    }

    char32_t *write_pos = ts->field_buffer + ts->field_buffer_pos;
    for (; chunk_start < chunk_end; chunk_start++, write_pos++) {
        *write_pos = (char32_t)*chunk_start;
    }
    *write_pos = '\0';  /* always ensure we end with NUL */
    ts->field_buffer_pos += chunk_length;
    return 0;
}
/**end repeat**/


static NPY_INLINE int
add_field(tokenizer_state *ts)
{
    /* The previous field is done, advance to keep a NUL byte at the end */
    ts->field_buffer_pos += 1;

    size_t size = ts->num_fields + 1;
    if (NPY_UNLIKELY(size > ts->fields_size)) {
        size = next_size(size);
        field_info *fields = PyMem_Realloc(ts->fields, size * sizeof(*fields));
        if (fields == NULL) {
            PyErr_NoMemory();
            return -1;
        }
        ts->fields = fields;
        ts->fields_size = size;
    }

    ts->fields[ts->num_fields].offset = ts->field_buffer_pos;
    ts->fields[ts->num_fields].quoted = false;
    ts->num_fields += 1;
    /* Ensure this (currently empty) word is NUL terminated. */
    ts->field_buffer[ts->field_buffer_pos] = '\0';
    return 0;
}


/*
 * This version now always copies the full "row" (all tokens).  This makes
 * two things easier:
 * 1. It means that every word is guaranteed to be followed by a NUL character
 *    (although it can include one as well).
 * 2. In the usecols case we can sniff the first row easier by parsing it
 *    fully.
 *
 * The tokenizer could grow the ability to skip fields and check the
 * maximum number of fields when known.
 *
 * Unlike other tokenizers, this one tries to work in chunks and copies
 * data to words only when it it has to.  The hope is that this makes multiple
 * light-weight loops rather than a single heavy one, to allow e.g. quickly
 * scanning for the end of a field.
 */
int
tokenize(stream *s, tokenizer_state *ts, parser_config *const config)
{
    assert(ts->fields_size >= 2);
    assert(ts->field_buffer_length >= 2*sizeof(char32_t));

    int finished_reading_file = 0;

    /* Reset to start of buffer */
    ts->field_buffer_pos = 0;
    ts->num_fields = 0;
    /* Add the first field */

    while (1) {
        if (ts->state == TOKENIZE_INIT) {
            /* Start a new field */
            if (add_field(ts) < 0) {
                return -1;
            }
            ts->state = TOKENIZE_CHECK_QUOTED;
        }

        if (NPY_UNLIKELY(ts->pos >= ts->end)) {
            if (ts->buf_state == BUFFER_IS_LINEND &&
                    ts->state == TOKENIZE_EAT_CRLF) {
                /* Finished line, do not read anymore (also do not eat \n) */
                goto finish;
            }
            /* fetch new data */
            ts->buf_state = stream_nextbuf(s,
                    &ts->pos, &ts->end, &ts->unicode_kind);
            if (ts->buf_state < 0) {
                return -1;
            }
            if (ts->buf_state == BUFFER_IS_FILEEND) {
                finished_reading_file = 1;
                ts->pos = ts->end;  /* should be guaranteed, but make sure. */
                goto finish;
            }
            else if (ts->pos == ts->end) {
                if (ts->buf_state != BUFFER_IS_LINEND) {
                    PyErr_SetString(PyExc_RuntimeError,
                            "Reader returned an empty buffer, "
                            "but did not indicate file or line end.");
                    return -1;
                }
                /* Otherwise, we are OK with this and assume an empty line. */
                goto finish;
            }
        }
        switch (ts->unicode_kind) {
        /**begin repeat
         * #kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND, PyUnicode_4BYTE_KIND#
         * #type = Py_UCS1, Py_UCS2, Py_UCS4#
         */
        case @kind@:
        {
            @type@ *pos = (@type@ *)ts->pos;
            @type@ *stop = (@type@ *)ts->end;
            @type@ *chunk_start;

            if (ts->state == TOKENIZE_CHECK_QUOTED) {
                /* before we can check for quotes, strip leading spaces */
                if (config->ignore_leading_spaces) {
                    while (pos < stop && *pos == ' ') {
                        pos++;
                    }
                    if (pos == stop) {
                        ts->pos = (char *)pos;
                        break;
                    }
                }

                /* Setting chunk effectively starts the field */
                if (*pos == config->quote) {
                    ts->fields[ts->num_fields - 1].quoted = true;
                    ts->state = TOKENIZE_QUOTED;
                    pos++;  /* TOKENIZE_QUOTED is OK with pos == stop */
                }
                else {
                    ts->state = TOKENIZE_UNQUOTED;
                }
            }

            switch (ts->state) {
                case TOKENIZE_UNQUOTED:
                    chunk_start = pos;
                    for (; pos < stop; pos++) {
                        if (*pos == '\n') {
                            ts->state = TOKENIZE_EAT_CRLF;
                            break;
                        }
                        else if (*pos == '\r') {
                            ts->state = TOKENIZE_LINE_END;
                            break;
                        }
                        else if (*pos == config->delimiter) {
                            ts->state = TOKENIZE_INIT;
                            break;
                        }
                        else if (*pos == config->comment[0]) {
                            if (config->comment[1] != '\0') {
                                ts->state = TOKENIZE_CHECK_COMMENT;
                                break;
                            }
                            else {
                                ts->state = TOKENIZE_GOTO_LINE_END;
                                break;
                            }
                        }
                    }
                    if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {
                        return -1;
                    }
                    pos++;
                    break;

                case TOKENIZE_QUOTED:
                    chunk_start = pos;
                    for (; pos < stop; pos++) {
                        if (!config->allow_embedded_newline) {
                            if (*pos == '\n') {
                                ts->state = TOKENIZE_EAT_CRLF;
                                break;
                            }
                            else if (*pos == '\r') {
                                ts->state = TOKENIZE_LINE_END;
                                break;
                            }
                        }
                        else if (*pos != config->quote) {
                            /* inside the field, nothing to do. */
                        }
                        else {
                            ts->state = TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE;
                            break;
                        }
                    }
                    if (copy_to_field_buffer_@type@(ts, chunk_start, pos) < 0) {
                        return -1;
                    }
                    pos++;
                    break;

                case TOKENIZE_CHECK_COMMENT:
                    if (*pos == config->comment[1]) {
                        ts->state = TOKENIZE_GOTO_LINE_END;
                        pos++;
                    }
                    else {
                        /* Not a comment, must be tokenizing unquoted now */
                        ts->state = TOKENIZE_UNQUOTED;
                        /* Copy comment as a chunk to the current field */
                        if (copy_to_field_buffer_Py_UCS4(ts,
                                config->comment, config->comment+1) < 0) {
                            return -1;
                        }
                    }
                    break;

                case TOKENIZE_QUOTED_CHECK_DOUBLE_QUOTE:
                    if (*pos == config->quote) {
                        ts->state = TOKENIZE_QUOTED;
                        pos++;
                    }
                    else {
                        /* continue parsing as if unquoted */
                        ts->state = TOKENIZE_UNQUOTED;
                    }
                    break;

                case TOKENIZE_GOTO_LINE_END:
                    if (ts->buf_state != BUFFER_MAY_CONTAIN_NEWLINE) {
                        ts->pos = ts->end;  /* advance to next buffer */
                        goto finish;
                    }
                    for (; pos < stop; pos++) {
                        if (*pos == '\r' || *pos == '\n') {
                            ts->state = TOKENIZE_EAT_CRLF;
                            break;
                        }
                    }
                    break;

                case TOKENIZE_EAT_CRLF:
                    /* "Universal newline" support: remove \n in \r\n. */
                    if (*pos == '\n') {
                        pos++;
                    }
                    /* We are done for good if we reached here */
                    ts->pos = (char *)pos;
                    goto finish;

                default:
                    assert(0);
            }

            ts->pos = (char *)pos;
            break;
        }
            /**end repeat**/
            default:
                assert(0);  /* should not happen, line end is transitional */
        }

        if (ts->state == TOKENIZE_LINE_END) {
            goto finish;
        }
    }

  finish:
    /* Finish the last field */
    if (add_field(ts) < 0) {
        return -1;
    }
    ts->num_fields -= 1;
    /*
     * If have one field, but that field is completely empty, this is an
     * empty line, and we just ignore it.
     */
    if (ts->num_fields == 1
             && ts->fields[1].offset - ts->fields[0].offset == 1
             && !ts->fields->quoted) {
        ts->num_fields--;
    }
    ts->state = TOKENIZE_INIT;
    return finished_reading_file;
}


void
tokenizer_clear(tokenizer_state *ts)
{
    PyMem_FREE(ts->field_buffer);
    ts->field_buffer = NULL;
    ts->field_buffer_length = 0;

    PyMem_FREE(ts->fields);
    ts->fields = NULL;
    ts->fields_size = 0;
}


int
tokenizer_init(tokenizer_state *ts, parser_config *config)
{
    /* State and buf_state could be moved into tokenize if we go by row */
    ts->buf_state = BUFFER_MAY_CONTAIN_NEWLINE;
    ts->state = TOKENIZE_INIT;
    ts->num_fields = 0;

    ts->buf_state = 0;
    ts->pos = NULL;
    ts->end = NULL;

    ts->field_buffer = PyMem_Malloc(32 * sizeof(char32_t));
    if (ts->field_buffer == NULL) {
        PyErr_NoMemory();
        return -1;
    }
    ts->field_buffer_length = 32;

    ts->fields = PyMem_Malloc(5 * sizeof(*ts->fields));
    if (ts->fields == NULL) {
        PyErr_NoMemory();
        return -1;
    }
    ts->fields_size = 5;
    return 0;
}
